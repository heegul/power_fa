#!/usr/bin/env python3
"""
Network Capacity Study

This script investigates the memorization capacity of various DNN architectures 
by varying the number of training samples. It evaluates the models on the training 
data itself to measure how well they can memorize the dataset.

Key features:
- Sweeps through different numbers of training samples.
- Sweeps through various DNN hidden layer configurations.
- For each run, num_train_samples = num_val_samples = N (from sweep set).
- Each run uses a new, non-overlapping set of N samples for both training and validation.
- Trains a new model for each run and saves it in the run directory.
- Optimally, this should be used with a cluster management tool like Slurm
  to parallelize the runs.
"""
import os
import subprocess
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import argparse
import glob
import sys
import math
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed

# Try to import torch and set default device, but don't fail if not installed
# as this script can also be used for post-processing/plotting only.
try:
    import torch
    DEFAULT_TORCH_DEVICE = 'cuda' if torch.cuda.is_available() else (
        'mps' if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available() else 'cpu')
except ImportError:
    DEFAULT_TORCH_DEVICE = 'cpu'

# --------------------------------------------------------------------------------------
# Helper: ensure the required environment .pkl file exists (generates it if missing)
# --------------------------------------------------------------------------------------

def _ensure_env_file(num_samples: int, num_pairs: int = 6):
    """Create the `environment_samples_db_pairs{num_pairs}_samples{num_samples}_rxFalse.pkl`
    file on-the-fly if it does not already exist.

    The file is generated by invoking ``dnn_d2d_pytorch.py`` in *environment-generation* mode
    (``--epochs 0``).  The generated file is placed in the current working directory so
    that all subsequent scripts can discover it with a simple relative path.
    """

    fname = f"environment_samples_db_pairs{num_pairs}_samples{num_samples}_rxFalse.pkl"

    # Absolute path ensures downstream subprocesses can locate the file even if
    # they change the working directory (e.g. when writing figures to a
    # different folder).
    abs_path = os.path.abspath(fname)

    if os.path.exists(abs_path):
        return abs_path  # Already present ‚Äì nothing to do.

    print(f"üõ†  Environment file not found ‚Äì generating {fname} ...")

    gen_cmd = [
        sys.executable,
        "dnn_d2d_pytorch.py",
        "--num_samples",
        str(num_samples),
        "--num_pairs",
        str(num_pairs),
        "--epochs",
        "0",  # generation-only mode
        "--figs_dir",
        ".",  # save .pkl in CWD so NetworkCapacityStudy can find it
    ]

    try:
        subprocess.run(gen_cmd, check=True)
        print("   ‚úì Environment file generated successfully.")
    except subprocess.CalledProcessError as e:
        print("   ‚ö†Ô∏è  Failed to generate environment file. Command output:")
        print(e.stdout)
        print(e.stderr)
        raise

    return abs_path

class NetworkCapacityStudy:
    def __init__(self, base_dir=None, target_ratios=1, generalization_eval=False, eval_only_batch=False, generalization_only=False, samples_filter=None, hidden_filter=None, force_rerun=False, fa_only_mode=False, num_frequencies=3, use_gumbel=False, gumbel_temp=1.0, dropout_p=0.0, weight_decay=0.0, epochs=None, lr=None, force_num_samples_eval=None, max_parallel_runs: int = 1):
        """
        Initializes the study.
        
        Args:
            base_dir (str, optional): The base directory for results. If None, a timestamped
                                      directory is created. Defaults to None.
            target_ratios (int, optional): The number of independent runs to average for each
                                           hyperparameter configuration. Defaults to 1.
            generalization_eval (bool, optional): If True, also run a generalization test on
                                                  a fixed, larger dataset. Defaults to False.
            eval_only_batch (bool, optional): If True, only runs the final analysis and plotting on
                                                pre-existing data. Defaults to False.
            generalization_only (bool, optional): If True, only runs generalization evaluation on
                                                  existing trained models. Defaults to False.
            samples_filter (callable, optional): A function to filter training samples. Defaults to None.
            hidden_filter (callable, optional): A function to filter hidden layer configurations. Defaults to None.
            force_rerun (bool, optional): If True, forces re-run of all experiments. Defaults to False.
            fa_only_mode (bool, optional): If True, enable FA-only experiments (fixed max TX power). Defaults to False.
            num_frequencies (int, optional): Number of available frequencies when FA-only mode is enabled (must be >1). Defaults to 3.
            use_gumbel (bool, optional): If True, use Gumbel-Softmax FA. Defaults to False.
            gumbel_temp (float, optional): Temperature for Gumbel-Softmax FA. Defaults to 1.0.
            dropout_p (float, optional): Dropout probability for DNN hidden layers. Defaults to 0.0.
            weight_decay (float, optional): L2 weight decay for optimiser. Defaults to 0.0.
            epochs (int, optional): Number of training epochs for DNN. Defaults to None.
            lr (float, optional): Learning rate for DNN optimizer. Defaults to None.
            force_num_samples_eval (int, optional): Force number of evaluation samples for generalization. Defaults to None.
            max_parallel_runs (int, optional): Maximum number of parallel runs. Defaults to 1.
        """
        if base_dir is None:
            timestamp = datetime.now().strftime("%m%d")
            self.base_dir = os.path.join('results', f'capacity_study_{timestamp}')
        else:
            self.base_dir = base_dir
        
        self.results_file = os.path.join(self.base_dir, 'capacity_study_results.json')
        self.results = self._load_results()
        self.target_ratios = target_ratios
        
        # --- Experiment Parameters ---
        self.num_train_samples_sweep = [1, 2, 5, 10, 100]
#        self.num_train_samples_sweep = [1000, 10000]
        self.hidden_size_sweep = [
#            [20],
            [100],
#            [1000],
            [200, 200],
#            [600, 600, 600],
#           [1000, 1000, 1000],
            [500, 500, 500, 500],
        ]
        self.generalization_eval = generalization_eval
        self.eval_only_batch = eval_only_batch
        self.generalization_only = generalization_only
        
        # --- Training Settings ---
        self.epochs = epochs
        self.batch_size_small_n = 10 # Batch size for N < 200
        self.batch_size_large_n = 200 # Batch size for N >= 200
        self.early_patience = 300

        # --- Optional filtering of scenarios ---
        # samples_filter: list[int] or None
        # hidden_filter: list[str] (string representation of hidden sizes) or None
        if samples_filter is not None:
            # Ensure list of ints
            self.num_train_samples_sweep = [n for n in self.num_train_samples_sweep if n in samples_filter]

        if hidden_filter is not None:
            # Override hidden size sweep with the user-provided configurations
            self.hidden_size_sweep = []
            for hid_str in hidden_filter:
                cfg = [int(h) for h in hid_str.split('_') if h]
                self.hidden_size_sweep.append(cfg)

        # Store force_rerun flag
        self.force_rerun = force_rerun

        # Store FA-only mode and number of frequencies
        self.fa_only_mode = fa_only_mode
        self.num_frequencies = num_frequencies
        self.use_gumbel = use_gumbel
        self.gumbel_temp = gumbel_temp
        self.dropout_p = dropout_p
        self.weight_decay = weight_decay
        self.lr = lr
        self.force_num_samples_eval = force_num_samples_eval

        # Parallelism
        self.max_parallel_runs = max(1, int(max_parallel_runs))

    def _load_results(self):
        """Loads existing results from the JSON file, or returns an empty dict."""
        if os.path.exists(self.results_file):
            print(f"üìÅ Results loaded from {self.results_file}")
            with open(self.results_file, 'r') as f:
                return json.load(f)
        else:
            print(f"‚ùå Results file not found: {self.results_file}")
            return {}

    def _save_results(self):
        """Saves the current results to the JSON file."""
        os.makedirs(self.base_dir, exist_ok=True)
        with open(self.results_file, 'w') as f:
            json.dump(self.results, f, indent=4)
        print(f"üìÅ Aggregated results saved to {self.results_file}")

    def _get_batch_size(self, n_samples):
        """Returns an adaptive batch size based on the number of samples."""
        if n_samples < 200:
            return self.batch_size_small_n
        return self.batch_size_large_n

    def run_study(self):
        """Runs the entire study sweep."""
        print("üî¨ Starting Network Capacity Study")
        print(f"   - Base directory: {self.base_dir}")
        print(f"   - Generalization evaluation: {self.generalization_eval}")
        print(f"   - Eval only batch: {self.eval_only_batch}")

        # Preview what will be run
        configs_to_run = []
        for n in self.num_train_samples_sweep:
            for hidden_config in self.hidden_size_sweep:
                hidden_str = '_'.join(map(str, hidden_config))
                exp_name = f"samples_{n}_hidden_{hidden_str}"
                
                # Check how many runs are already completed
                run_dirs = glob.glob(os.path.join(self.base_dir, f"samples_{n}", f"hidden_{hidden_str}", "run_*"))
                num_completed = len(run_dirs)

                if num_completed < self.target_ratios:
                    configs_to_run.append(exp_name)

        print(f"‚úîÔ∏é Preview: {len(self.results) - len(configs_to_run)} configurations will be skipped, {len(configs_to_run)} will be (re)run.")
        for name in configs_to_run:
            print(f"   To run ‚Üí {name}")

        # Run experiments
        for n in self.num_train_samples_sweep:
            for hidden_config in self.hidden_size_sweep:
                self.run_experiment(hidden_config, n)

        print("\n‚úÖ Study complete.")
        self._save_results()

    def run_experiment(self, hidden_config, n):
        """
        Runs a single experiment for a given number of samples and hidden layer config.
        """
        hidden_str = '_'.join(map(str, hidden_config))
        exp_name = f"samples_{n}_hidden_{hidden_str}"

        # --------------------------------------------------------------------------
        # Ensure the required training/validation environment file exists
        # (dnn_d2d_pytorch.py expects 2*n samples for training+validation)
        # --------------------------------------------------------------------------

        env_file_path = _ensure_env_file(num_samples=2 * n, num_pairs=6)

        # Determine the number of training runs required based on the new logic
        if n >= self.target_ratios:
            num_runs_needed = 1
        else:
            num_runs_needed = math.ceil(self.target_ratios / n)

        # Check how many runs are already completed
        run_dirs = glob.glob(os.path.join(self.base_dir, f"samples_{n}", f"hidden_{hidden_str}", "run_*"))

        # If force_rerun is enabled, remove existing run directories to start fresh
        if self.force_rerun and run_dirs:
            print(f"   üîÑ FORCE RERUN enabled ‚Äì deleting {len(run_dirs)} existing run directories for {exp_name}")
            for d in run_dirs:
                try:
                    shutil.rmtree(d)
                except Exception as e:
                    print(f"      ‚ö†Ô∏è  Could not delete {d}: {e}")
            # Refresh run_dirs after deletion
            run_dirs = []

        num_completed = len(run_dirs)

        if num_completed >= num_runs_needed:
            print(f"‚úîÔ∏é Skipping {exp_name}: {num_completed}/{num_runs_needed} runs already complete.")
        else:
            print(f"\nüöÄ Running: {exp_name} ({num_completed}/{num_runs_needed} complete, running {num_runs_needed - num_completed} more)")

            def _launch_pair(run_idx, memo_cmd, gen_cmd):
                """Helper executed inside thread. Runs memorization then (optionally) generalization."""
                # Run memorization
                try:
                    subprocess.run(memo_cmd, check=True, capture_output=True, text=True)
                except Exception as e:
                    print(f"      - ERROR: Run {run_idx} memorization failed: {e}")
                    return False

                if gen_cmd is not None:
                    try:
                        subprocess.run(gen_cmd, check=True, capture_output=True, text=True)
                    except Exception as e:
                        print(f"      - ERROR: Run {run_idx} generalization failed: {e}")
                        # Even if generalization fails, we mark memorization done
                return True

            tasks = []

            # --- Build commands for all needed runs first ---
            for run_idx in range(num_completed + 1, num_runs_needed + 1):
                run_dir = os.path.join(self.base_dir, f"samples_{n}", f"hidden_{hidden_str}", f"run_{run_idx}")
                if os.path.exists(os.path.join(run_dir, 'memorization_results.json')):
                    print(f"   - Skipping run {run_idx}: already complete.")
                    continue

                os.makedirs(run_dir, exist_ok=True)
                model_path = os.path.join(run_dir, 'model.pt')
                device_val = 'cpu' if n in [1, 2] else DEFAULT_TORCH_DEVICE
                adaptive_bs = self._get_batch_size(n)

                memo_cmd = [
                    sys.executable, 'dnn_d2d_pytorch.py',
                    '--num_samples', str(2 * n), '--val_ratio', '0.5',
                    '--hidden_sizes', *[str(s) for s in hidden_config],
                    '--num_frequencies', str(self.num_frequencies), '--use_train_for_val',
                    '--enable_fs', '--batch_size', str(adaptive_bs), '--figs_dir', run_dir,
                    '--early_stop_patience', str(self.early_patience), '--early_stop_min_delta', '0.0001',
                    '--device', device_val
                ]
                if self.fa_only_mode:
                    memo_cmd.append('--fa_only_mode')
                else:
                    memo_cmd.append('--discrete_power')
                if self.use_gumbel:
                    memo_cmd.extend(['--fa_gumbel_softmax', '--fa_gumbel_temp', str(self.gumbel_temp)])
                if self.dropout_p > 0.0:
                    memo_cmd.extend(['--dropout_p', str(self.dropout_p)])
                if self.weight_decay > 0.0:
                    memo_cmd.extend(['--weight_decay', str(self.weight_decay)])
                if self.epochs is not None:
                    memo_cmd.extend(['--epochs', str(self.epochs)])
                if self.lr is not None:
                    memo_cmd.extend(['--lr', str(self.lr)])

                gen_cmd = None
                if self.generalization_eval or self.eval_only_batch:
                    gen_run_dir = os.path.join(run_dir, 'generalization'); os.makedirs(gen_run_dir, exist_ok=True)
                    unique_seed = hash(f"{hidden_str}_{n}_{run_idx}") % 10000
                    gen_cmd = [
                        sys.executable, 'dnn_d2d_pytorch.py',
                        '--eval_only', '--load_model_path', model_path,
                        '--train_gain_file', env_file_path,
                        '--num_frequencies', str(self.num_frequencies),
                        '--hidden_sizes', *[str(s) for s in hidden_config],
                        '--enable_fs', '--figs_dir', gen_run_dir, '--device', device_val,
                        '--seed', str(unique_seed)
                    ]
                    if self.dropout_p > 0.0:
                        gen_cmd.extend(['--dropout_p', str(self.dropout_p)])
                    if self.weight_decay > 0.0:
                        gen_cmd.extend(['--weight_decay', str(self.weight_decay)])
                    if self.epochs is not None:
                        gen_cmd.extend(['--epochs', str(self.epochs)])
                    if self.lr is not None:
                        gen_cmd.extend(['--lr', str(self.lr)])
                    if self.force_num_samples_eval is not None:
                        gen_cmd.extend(['--num_eval_samples', str(self.force_num_samples_eval)])
                    if self.fa_only_mode:
                        gen_cmd.append('--fa_only_mode')
                    else:
                        gen_cmd.append('--discrete_power')
                    if self.use_gumbel:
                        gen_cmd.extend(['--fa_gumbel_softmax', '--fa_gumbel_temp', str(self.gumbel_temp)])

                print(f"   - Queueing run {run_idx}/{num_runs_needed}")
                tasks.append((run_idx, memo_cmd, gen_cmd))

            # --- Execute with thread pool ---
            with ThreadPoolExecutor(max_workers=self.max_parallel_runs) as pool:
                futures = [pool.submit(_launch_pair, rid, mcmd, gcmd) for (rid, mcmd, gcmd) in tasks]
                for fut in as_completed(futures):
                    fut.result()  # Exceptions already printed inside helper

        # --- Aggregate and truncate results from all runs for this experiment ---
        all_mem_ratios, all_gen_ratios = [], []
        
        final_run_dirs = sorted(glob.glob(os.path.join(self.base_dir, f"samples_{n}", f"hidden_{hidden_str}", "run_*")))
        
        for run_dir in final_run_dirs:
            try:
                mem_ratio_file = os.path.join(run_dir, 'memorization_results.json')
                if os.path.exists(mem_ratio_file):
                    with open(mem_ratio_file, 'r') as f:
                        all_mem_ratios.extend(json.load(f)['ratios'])

                gen_results_file = os.path.join(run_dir, 'generalization', 'generalisation_results.json')
                if os.path.exists(gen_results_file):
                    with open(gen_results_file, 'r') as f:
                        all_gen_ratios.extend(json.load(f)['ratios'])
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"   - WARNING: Could not load results from {run_dir}, skipping. Error: {e}")

        # Truncate the lists to the target number of ratios
        final_mem_ratios = all_mem_ratios[:self.target_ratios]
        final_gen_ratios = all_gen_ratios[:self.target_ratios]

        # Update results for this configuration
        if exp_name not in self.results: self.results[exp_name] = {}
        
        if final_mem_ratios:
            self.results[exp_name].update({
                'memorization_ratios': final_mem_ratios,
                'memorization_avg': np.mean(final_mem_ratios),
                'memorization_std': np.std(final_mem_ratios)
            })

        if final_gen_ratios:
            self.results[exp_name].update({
                'generalization_ratios': final_gen_ratios,
                'generalization_avg': np.mean(final_gen_ratios),
                'generalization_std': np.std(final_gen_ratios)
            })

        mem_msg = f"Mem Avg: {np.mean(final_mem_ratios):.4f}" if final_mem_ratios else "Mem: N/A"
        gen_msg = f", Gen Avg: {np.mean(final_gen_ratios):.4f}" if final_gen_ratios else ""
        print(f"   - ‚úÖ Completed & Aggregated. {mem_msg}{gen_msg}")
        self._save_results()

    def run_generalization_only(self):
        """Run generalization evaluation only on existing trained models.
        
        This method will:
        1. Scan for all existing trained models
        2. Force re-run generalization evaluation (overriding existing results)
        3. Use unique seeds for each configuration to ensure different test data
        4. Update capacity_study_results.json with new generalization results
        """
        
        print("üîç Running GENERALIZATION-ONLY evaluation on existing models...")
        print("‚ö†Ô∏è  WARNING: This will OVERRIDE any existing generalization results!")
        print(f"   Base directory: {self.base_dir}")
        print(f"   Target ratios per configuration: {self.target_ratios}")
        print("=" * 80)
        
        if not os.path.exists(self.base_dir):
            print(f"‚ùå Base directory not found: {self.base_dir}")
            return
        
        # Find all existing model directories
        existing_configs = []
        for n in self.num_train_samples_sweep:
            for hidden_config in self.hidden_size_sweep:
                hidden_str = "_".join(map(str, hidden_config))
                exp_name = f"samples_{n}_hidden_{hidden_str}"
                
                # Look for existing run directories with trained models
                config_dir = os.path.join(self.base_dir, f"samples_{n}", f"hidden_{hidden_str}")
                if os.path.exists(config_dir):
                    run_dirs = glob.glob(os.path.join(config_dir, "run_*"))
                    model_dirs = []
                    
                    for run_dir in run_dirs:
                        model_path = os.path.join(run_dir, "model.pt")
                        if os.path.exists(model_path):
                            model_dirs.append(run_dir)
                    
                    if model_dirs:
                        existing_configs.append({
                            'exp_name': exp_name,
                            'n': n,
                            'hidden_config': hidden_config,
                            'hidden_str': hidden_str,
                            'model_dirs': model_dirs
                        })
        
        if not existing_configs:
            print("‚ùå No existing trained models found!")
            return
        
        print(f"‚úÖ Found {len(existing_configs)} configurations with trained models")
        
        # Process each configuration
        for config in existing_configs:
            exp_name = config['exp_name']
            n = config['n']
            hidden_config = config['hidden_config']
            hidden_str = config['hidden_str']
            model_dirs = config['model_dirs']
            
            print(f"\nüöÄ Processing: {exp_name} ({len(model_dirs)} trained models)")
            
            all_gen_ratios = []
            successful_runs = 0
            
            for run_dir in model_dirs:
                run_id = os.path.basename(run_dir).split('_')[1]
                model_path = os.path.join(run_dir, "model.pt")
                gen_run_dir = os.path.join(run_dir, 'generalization')
                
                # Force re-run generalization (override existing results)
                gen_results_file = os.path.join(gen_run_dir, 'generalisation_results.json')
                if os.path.exists(gen_results_file):
                    print(f"   üîÑ Run {run_id}: overriding existing generalization results")
                
                # Run generalization evaluation
                os.makedirs(gen_run_dir, exist_ok=True)
                
                # Generate unique seed for each configuration and run
                unique_seed = hash(f"{hidden_str}_{n}_{run_id}") % 10000
                
                device_val = 'cpu' if n in [1, 2] else DEFAULT_TORCH_DEVICE
                
                env_file_path = _ensure_env_file(num_samples=2 * n, num_pairs=6)
                gen_cmd = [
                    sys.executable, 'dnn_d2d_pytorch.py',
                    '--eval_only', '--load_model_path', model_path,
                    '--train_gain_file', env_file_path,
                    '--num_eval_samples', str(n),
                    '--num_frequencies', str(self.num_frequencies),
                    '--hidden_sizes', *[str(s) for s in hidden_config],
                    '--enable_fs', '--figs_dir', gen_run_dir, '--device', device_val,
                    '--seed', str(unique_seed)
                ]

                if self.dropout_p > 0.0:
                    gen_cmd.extend(['--dropout_p', str(self.dropout_p)])
                if self.weight_decay > 0.0:
                    gen_cmd.extend(['--weight_decay', str(self.weight_decay)])

                if self.epochs is not None:
                    gen_cmd.extend(['--epochs', str(self.epochs)])
                if self.lr is not None:
                    gen_cmd.extend(['--lr', str(self.lr)])
                if self.force_num_samples_eval is not None:
                    gen_cmd.extend(['--num_eval_samples', str(self.force_num_samples_eval)])

                if self.fa_only_mode:
                    gen_cmd.append('--fa_only_mode')
                else:
                    gen_cmd.append('--discrete_power')

                if self.use_gumbel:
                    gen_cmd.extend(['--fa_gumbel_softmax', '--fa_gumbel_temp', str(self.gumbel_temp)])

                print(f"   üîç Run {run_id}: running generalization (seed={unique_seed})")
                try:
                    subprocess.run(gen_cmd, check=True, capture_output=True, text=True)
                    
                    # Load the generated results
                    with open(gen_results_file, 'r') as f:
                        gen_data = json.load(f)
                    all_gen_ratios.extend(gen_data['ratios'])
                    successful_runs += 1
                    
                except subprocess.CalledProcessError as e:
                    print(f"   ‚ùå Run {run_id}: generalization failed")
                    if e.stdout:
                        print(f"      stdout: {e.stdout[:200]}...")
                    if e.stderr:
                        print(f"      stderr: {e.stderr[:200]}...")
                except Exception as e:
                    print(f"   ‚ùå Run {run_id}: error: {e}")
            
            # Update results with generalization data
            if all_gen_ratios:
                # Truncate to target ratios
                final_gen_ratios = all_gen_ratios[:self.target_ratios]
                
                # Update or create results entry
                if exp_name not in self.results:
                    self.results[exp_name] = {}
                
                self.results[exp_name].update({
                    'generalization_ratios': final_gen_ratios,
                    'generalization_avg': np.mean(final_gen_ratios),
                    'generalization_std': np.std(final_gen_ratios)
                })
                
                print(f"   ‚úÖ Updated with {len(final_gen_ratios)} generalization ratios")
                print(f"      Mean: {np.mean(final_gen_ratios):.4f} ¬± {np.std(final_gen_ratios):.4f}")
            else:
                print(f"   ‚ùå No successful generalization runs for {exp_name}")
        
        # Save updated results
        self._save_results()
        print(f"\nüíæ Updated results saved to: {self.results_file}")
        print(f"üéâ Generalization-only evaluation complete!")
        print(f"   ‚úÖ Processed {len(existing_configs)} configurations")
        print(f"   üîÑ All generalization results have been re-computed with unique test data")

    def analyze_results(self, print_tables: bool = False):
        """Loads results and generates plots."""
        if not self.results:
            print("‚ùå No results to analyze. Please run the study first.")
            return

        print("üìä Generating plots/tables...")
        
        # Convert results to a pandas DataFrame for easier analysis
        data = []
        for exp_name, res in self.results.items():
            parts = exp_name.split('_')
            num_samples = int(parts[1])
            hidden_str = '_'.join(parts[3:])
            
            row = {
                'num_samples': num_samples,
                'hidden_str': hidden_str,
                'memorization_avg': res.get('memorization_avg'),
                'memorization_std': res.get('memorization_std'),
                'generalization_avg': res.get('generalization_avg'),
                'generalization_std': res.get('generalization_std')
            }
            data.append(row)
        
        df = pd.DataFrame(data).sort_values(by=['hidden_str', 'num_samples'])
        
        # --- Plotting ---
        sns.set_theme(style="whitegrid")

        # Plot 1: Memorization vs. Number of Samples
        fig, ax = plt.subplots(figsize=(12, 7))
        sns.lineplot(data=df, x='num_samples', y='memorization_avg', hue='hidden_str', 
                     marker='o', ax=ax, palette='viridis')
        ax.set_xscale('log')
        ax.set_title('Memorization Performance vs. Number of Training Samples')
        ax.set_xlabel('Number of Samples (N)')
        ax.set_ylabel('DNN/FS Sum-Rate Ratio (Memorization)')
        ax.axhline(1.0, color='r', linestyle='--', label='Optimal (FS)')
        ax.legend(title='Hidden Layers', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(self.base_dir, 'memorization_vs_samples.png'), dpi=300)
        plt.savefig(os.path.join(self.base_dir, 'memorization_vs_samples.eps'), format='eps')
        plt.close()

        # Plot 2: Memorization vs. Network Size (for a fixed number of samples)
        # We can create a plot for each N value
        for n_val in df['num_samples'].unique():
            subset = df[df['num_samples'] == n_val]
            if len(subset) > 1:
                fig, ax = plt.subplots(figsize=(10, 6))
                # Create a pseudo-network size for x-axis
                subset['network_size_cat'] = subset['hidden_str']
                sns.pointplot(data=subset, x='network_size_cat', y='memorization_avg', ax=ax, capsize=.2)
                ax.set_title(f'Memorization vs. Network Size (N = {n_val})')
                ax.set_xlabel('Hidden Layer Configuration')
                ax.set_ylabel('DNN/FS Sum-Rate Ratio (Memorization)')
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                plt.savefig(os.path.join(self.base_dir, f'memorization_vs_size_N_{n_val}.png'), dpi=300)
                plt.savefig(os.path.join(self.base_dir, f'memorization_vs_size_N_{n_val}.eps'), format='eps')
                plt.close()
        
        # Plot 3: Generalization (if enabled)
        if (self.generalization_eval or self.eval_only_batch or self.generalization_only) and 'generalization_avg' in df.columns:
            # Generalization vs. Number of Samples
            fig, ax = plt.subplots(figsize=(12, 7))
            sns.lineplot(data=df, x='num_samples', y='generalization_avg', hue='hidden_str', 
                         marker='o', ax=ax, palette='viridis')
            ax.set_xscale('log')
            ax.set_title('Generalization Performance vs. Number of Training Samples')
            ax.set_xlabel('Number of Samples (N)')
            ax.set_ylabel('DNN/FS Sum-Rate Ratio (Generalization)')
            ax.axhline(1.0, color='r', linestyle='--', label='Optimal (FS)')
            ax.legend(title='Hidden Layers', bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()
            plt.savefig(os.path.join(self.base_dir, 'generalization_vs_samples.png'), dpi=300)
            plt.savefig(os.path.join(self.base_dir, 'generalization_vs_samples.eps'), format='eps')
            plt.close()

        # --- Optional: print LaTeX tables ---
        if print_tables:
            print("\n===== LaTeX Table: Memorization Average =====")
            mem_tbl = df.pivot(index='num_samples', columns='hidden_str', values='memorization_avg').sort_index()
            print(mem_tbl.to_latex(float_format="%.3f"))

            if self.generalization_eval or self.eval_only_batch or self.generalization_only:
                print("===== LaTeX Table: Generalization Average =====")
                gen_tbl = df.pivot(index='num_samples', columns='hidden_str', values='generalization_avg').sort_index()
                print(gen_tbl.to_latex(float_format="%.3f"))

        print(f"   - Plots (and tables) saved to {self.base_dir}")

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(description='Run a study on DNN memorization capacity.')
    parser.add_argument('--target_ratios', type=int, default=100, help='The number of independent runs to average for each hyperparameter configuration.')
    parser.add_argument('--generalization_eval', action='store_true', help='If set, also run a generalization test on a fixed, larger dataset.')
    parser.add_argument('--eval_only_batch', action='store_true', help='If set, only runs the final analysis and plotting on pre-existing data.')
    parser.add_argument('--generalization_only', action='store_true', help='If set, only runs generalization evaluation on existing trained models.')
    parser.add_argument('--base_dir', type=str, default=None, help='Base directory for results.')
    parser.add_argument('--plot_table_only', action='store_true', help='If set, only generate plots and LaTeX tables from existing results.')
    parser.add_argument('--filter_samples', type=str, default=None, help='Comma-separated list of N values (e.g., "1,2,5") to (re)run.')
    parser.add_argument('--filter_hidden', type=str, default=None, help='Comma-separated list of hidden configurations (e.g., "20,200_200") to (re)run.')
    parser.add_argument('--force_rerun', action='store_true', help='Force re-run even if results already exist.')
    parser.add_argument('--fa_only_mode', action='store_true', help='Enable FA-only experiments (fixed max TX power).')
    parser.add_argument('--num_frequencies', type=int, default=3, help='Number of available frequencies when FA-only mode is enabled (must be >1).')
    parser.add_argument('--use_gumbel', action='store_true', help='Use Gumbel-Softmax FA.')
    parser.add_argument('--gumbel_temp', type=float, default=1.0, help='Temperature for Gumbel-Softmax FA.')
    parser.add_argument('--dropout_p', type=float, default=0.0, help='Dropout probability for DNN hidden layers.')
    parser.add_argument('--weight_decay', type=float, default=0.0, help='L2 weight decay for optimiser.')
    parser.add_argument('--epochs', type=int, default=None, help='Number of training epochs for DNN.')
    parser.add_argument('--lr', type=float, default=None, help='Learning rate for DNN optimizer.')
    parser.add_argument('--force_num_samples_eval', type=int, default=None, help='Force number of evaluation samples for generalization.')
    parser.add_argument('--max_parallel_runs', type=int, default=1, help='Maximum number of parallel runs.')
    args = parser.parse_args()

    # --- Parse filter arguments ---
    samples_filter_list = None
    if args.filter_samples:
        samples_filter_list = [int(x.strip()) for x in args.filter_samples.split(',') if x.strip()]

    hidden_filter_list = None
    if args.filter_hidden:
        hidden_filter_list = [x.strip() for x in args.filter_hidden.split(',') if x.strip()]

    # Helper to create study with common parameters
    def create_study(**kwargs):
        return NetworkCapacityStudy(base_dir=args.base_dir,
                                    target_ratios=args.target_ratios,
                                    samples_filter=samples_filter_list,
                                    hidden_filter=hidden_filter_list,
                                    force_rerun=args.force_rerun,
                                    fa_only_mode=args.fa_only_mode,
                                    num_frequencies=args.num_frequencies,
                                    use_gumbel=args.use_gumbel,
                                    gumbel_temp=args.gumbel_temp,
                                    dropout_p=args.dropout_p,
                                    weight_decay=args.weight_decay,
                                    epochs=args.epochs,
                                    lr=args.lr,
                                    force_num_samples_eval=args.force_num_samples_eval,
                                    max_parallel_runs=args.max_parallel_runs,
                                    **kwargs)

    if args.plot_table_only:
        study = create_study()
        # Force flags so analyze_results knows generalization data may exist
        study.generalization_eval = True
        study.eval_only_batch = True
        study.analyze_results(print_tables=True)
    elif args.eval_only_batch:
        study = create_study()
        study.analyze_results()
    elif args.generalization_only:
        study = create_study(generalization_only=True)
        study.run_generalization_only()
        study.analyze_results(print_tables=True)
    else:
        study = create_study(generalization_eval=args.generalization_eval)
        study.run_study()
        study.analyze_results(print_tables=False)

if __name__ == '__main__':
    main() 